# DistributedDataParallel PyTorch Template

## Overview
This repository contains a template for implementing DistributedDataParallel (DDP) with PyTorch. DDP enables easy and efficient parallelism without much code change, making it simpler to scale your models across multiple GPUs.

## Features
- Efficient multi-GPU training
- Easy to integrate with existing PyTorch models
- Example use-cases included
